{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8Un-rd3u3oF"
   },
   "source": [
    "# Use FB Prophet for Multivariate Time-series Forecasting: Six Group merchant for sum of all transactions\n",
    "\n",
    "Prophet is an open source library developed by Facebook which aims to make time-series forecasting easy and scalable. It is a type of a generalized additive model (GAM), which uses regression model with potentially non-linear smoothers. It is called additive because it addes multiple decomposed parts to explain some trends. For example, Prophet uses the following components: \n",
    "\n",
    "$$ y(t) = g(t) + s(t) + h(t) + e(t) $$\n",
    "\n",
    "where,  \n",
    "$g(t)$: Growth. Big trend. Non-periodic changes.   \n",
    "$s(t)$: Sesonality. Periodic changes (e.g. weekly, yearly, etc.) represented by Fourier Series.  \n",
    "$h(t)$: Holiday effect that represents irregular schedules.   \n",
    "$e(t)$: Error. Any idiosyncratic changes not explained by the model. \n",
    "\n",
    "# Table of Contents \n",
    "1. [Prepare Data](#prep)\n",
    "2. [Get Data](#data)\n",
    "3. [Data Processing](#processing)\n",
    "4. [Train Test Split](#split)\n",
    "5. [Baseline Model](#baseline)\n",
    "6. [Multivariate Model](#multivariate)\n",
    "7. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfPJby-2E_II"
   },
   "source": [
    "# Step 1: Install and Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cE2gt2RVPiu"
   },
   "source": [
    "<a id=prep></a>\n",
    "# 1. Prepare Data\n",
    "\n",
    "The goal of the time series model is to predict the six group merchant transactions. Prophet requires at least two columns as inputs: a ds column and a y column.\n",
    "\n",
    " * The ds column has the time information. Currently we have the date as the index, so we name this index as ds.\n",
    " * The y column has the time series transaction values. In this example, because we are predicting six group merchant transactions, the column name for the transactions is named y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaAXOq7cFUCx"
   },
   "outputs": [],
   "source": [
    "# Prophet model for time series forecast\n",
    "from prophet import Prophet\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "# import the math module\n",
    "import math\n",
    "import sklearn.metrics\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import miceforest as mf\n",
    "# Hyperparameter tuning\n",
    "import itertools\n",
    "import json\n",
    "from prophet.diagnostics import cross_validation, performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S3-Wup8FVRK"
   },
   "source": [
    "# Step 2: Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to change the file path\n",
    "#data_path = \"../../data/raw/Time_Series_Merchants_Transactions_Anonymized.csv\"\n",
    "#data_path = \"data/Time_Series_Merchants_Transactions_Anonymized.csv\"\n",
    "data_path = \"../data/ninety.csv\"\n",
    "#data_path = \"data/ninetyfive.csv\"\n",
    "df_merchant_transactions = pd.read_csv(data_path)\n",
    "merchant_names = df_merchant_transactions.iloc[:,0].values\n",
    "df_merchant_transactions = df_merchant_transactions.drop(columns='Merchant Name')\n",
    "my_input = ['ds', 'y'] \n",
    "my_input.extend(merchant_names) \n",
    "print(my_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing columns names with standard date format\n",
    "stddates = pd.date_range(start='2020-08', end='2022-10', freq=\"M\")\n",
    "df_merchant_transactions = pd.DataFrame(df_merchant_transactions.values)\n",
    "df_merchant_transactions = df_merchant_transactions.T\n",
    "#stddates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_sum = \"../data/data_clean_sum.csv\"\n",
    "df_merchant_transactions_sum = pd.read_csv(data_path_sum)\n",
    "df_merchant_transactions_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [df_merchant_transactions_sum,df_merchant_transactions]\n",
    "df_merchant_transactions_concat = pd.concat(ds, join='outer', axis=1)\n",
    "df_merchant_transactions_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHqhmZPEQt9h"
   },
   "source": [
    "# Step 3: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3YfRRwnxoIw"
   },
   "source": [
    "Step 3 transforms the dataset into a time series modeling dataset. \n",
    "\n",
    "\n",
    "Prophet requires at least two columns as inputs: a `ds` column and a `y` column.\n",
    "* The `ds` column has the time information. Currently we have the date as the index, so we reset the index and rename `date` to `ds`.\n",
    "* The y column has the time series values. In this example, because we are predicting the Google closing price, the column name `Merchant 1` is changed to `y`.\n",
    "* There is no pre-defined name for the additional predictor in prophet, so we can keep the names as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1653163975291,
     "user": {
      "displayName": "Amy Zhuang",
      "userId": "03652318964562397937"
     },
     "user_tz": 240
    },
    "id": "d_zo4cduo9c1",
    "outputId": "107a0deb-a753-4ba4-aee4-632e4193a3fb"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(df_merchant_transactions_concat.values)\n",
    "# Change variable names\n",
    "data.columns = my_input\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129,
     "status": "ok",
     "timestamp": 1653163978829,
     "user": {
      "displayName": "Amy Zhuang",
      "userId": "03652318964562397937"
     },
     "user_tz": 240
    },
    "id": "DL4Xkaprpho_",
    "outputId": "1381e9e0-7368-4fed-96f4-12c69e141dbb"
   },
   "outputs": [],
   "source": [
    "# Check correlation\n",
    "data_test = data.drop(columns='ds').apply(pd.to_numeric)\n",
    "correlation = data_test.corrwith(data_test['y'])#,numeric_only=True)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get heatmap of the correlation\n",
    "fig, ax = plt.subplots(figsize=(40, 30))\n",
    "sns.heatmap(data_test.corr(), ax=ax, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHFDb9YXZeIE"
   },
   "source": [
    "# Step 4: Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrUIIgp8DSlM"
   },
   "source": [
    "In step 4, we will do the train test split. For time series data, usually a threshold date is chosen, then we set the dates before the threshold to be the training dataset and the dates after the threshold to be the testing dataset.\n",
    "\n",
    "Based on the threshold date (`train_end_date`) we set before, there are data points in the training dataset and data points in the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1653163992111,
     "user": {
      "displayName": "Amy Zhuang",
      "userId": "03652318964562397937"
     },
     "user_tz": 240
    },
    "id": "s1n7NEZ2ToW_",
    "outputId": "a367fdc6-b6de-4a1f-96cd-58e25f1736de"
   },
   "outputs": [],
   "source": [
    "# Train test split the date need to be changed \n",
    "train_end_date = '2022-04-30'\n",
    "train = data[data['ds'] <= train_end_date]\n",
    "test = data[data['ds'] > train_end_date]\n",
    "\n",
    "# Check the shape of the dataset\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2x8mK5pE2n9"
   },
   "source": [
    "Checking the minimum and maximum values for the train and test dataset separately gave us the starting and ending dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1653163994673,
     "user": {
      "displayName": "Amy Zhuang",
      "userId": "03652318964562397937"
     },
     "user_tz": 240
    },
    "id": "pVFD6G__38hX",
    "outputId": "10ef9063-a64e-4659-c9f6-f32a22049b3f"
   },
   "outputs": [],
   "source": [
    "# Check the start and end time of the training and testing dataset\n",
    "print('The start time of the training dataset is ', train['ds'].min())\n",
    "print('The end time of the training dataset is ', train['ds'].max())\n",
    "print('The start time of the testing dataset is ', test['ds'].min())\n",
    "print('The end time of the testing dataset is ', test['ds'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ePfpgRtqKbc"
   },
   "source": [
    "# Step 5: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypamYI2zGdky"
   },
   "source": [
    "In step 5, we will build a univariate baseline model using the default prophet hyperparameters, and fit the model using the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyUdAGsLqVZc"
   },
   "source": [
    "## Step 5.1: Build Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4027,
     "status": "ok",
     "timestamp": 1653024223481,
     "user": {
      "displayName": "Amy Zhuang",
      "userId": "03652318964562397937"
     },
     "user_tz": 240
    },
    "id": "S1hnWrh7qllm",
    "outputId": "b7967b4c-00dd-4ff2-9d6d-e277a1eeeb87"
   },
   "outputs": [],
   "source": [
    "# Use the default hyperparameters to initiate the Prophet model\n",
    "model_baseline = Prophet(interval_width=0.99, seasonality_mode='multiplicative')\n",
    "\n",
    "# Fit the model on the training dataset\n",
    "model_baseline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kh97rZBJS5sA"
   },
   "source": [
    "Prophet automatically fits daily, weekly, and yearly seasonalities if the time series is more than two cycles long.\n",
    "\n",
    "The model information shows that the yearly seasonality and the daily seasonality are disabled. \n",
    "* The daily seasonality is disabled because we do not have sub-daily time series. \n",
    "* The yearly seasonality is disabled although we have two years of data. \n",
    "\n",
    "We will continue with the default values for the baseline model and force the yearly seasonality in the next model to see the impact of the yearly seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i25VpIgar-Kr"
   },
   "source": [
    "## Step 5.2: Baseline Model Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPg2YxA3Wpw6"
   },
   "source": [
    "After making the prediction on the future dataframe, we can plot the result using `.plot`.\n",
    "* The black dots are the actual values.\n",
    "* The blue line is the prediction.\n",
    "* The blue shades are the uncertainty interval. The default value for the uncertainty interval is 80%, so we are using 80% here. The uncertainty interval is calculated based on the assumption that the average frequency and magnitude of trend changes in the future will be the same as the historical data. The historical data trend changes are projected forward to get the uncertainty intervals [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "executionInfo": {
     "elapsed": 4365,
     "status": "ok",
     "timestamp": 1653024711353,
     "user": {
      "displayName": "Amy Zhuang",
      "userId": "03652318964562397937"
     },
     "user_tz": 240
    },
    "id": "wSVRPteNsZwX",
    "outputId": "e51bc246-abdf-4526-9751-860462bf31bc"
   },
   "outputs": [],
   "source": [
    "# Create the time range for the forecast\n",
    "future_baseline = model_baseline.make_future_dataframe(test.shape[0], freq='M')\n",
    "\n",
    "# Make prediction\n",
    "forecast_baseline = model_baseline.predict(future_baseline)\n",
    "\n",
    "# Visualize the forecast\n",
    "fig = model_baseline.plot(forecast_baseline); # Add semi-colon to remove the duplicated chart\n",
    "plt.legend(['Actual', 'Prediction', 'Uncertainty interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxrvuruXd5Bz"
   },
   "source": [
    "In addition to the forecast plot, prophet also provides the components plot. \n",
    "\n",
    "From the component plot chart, we can see that the no of transactions has an overall upward trend. The weekly seasonality shows that the price tends to be lower at the beginning of the week and higher at the end of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "executionInfo": {
     "elapsed": 914,
     "status": "ok",
     "timestamp": 1653024252965,
     "user": {
      "displayName": "Amy Zhuang",
      "userId": "03652318964562397937"
     },
     "user_tz": 240
    },
    "id": "fzIz3SSlsu_u",
    "outputId": "e2a19069-e0d7-4910-852f-8856a21f0503"
   },
   "outputs": [],
   "source": [
    "# Visualize the forecast components\n",
    "model_baseline.plot_components(forecast_baseline);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfLBCaN5tL5S",
    "tags": []
   },
   "source": [
    "## Step 6: Multivariate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(data, forecast):\n",
    "    # rmse = sqrt(sklearn.metrics.mean_squared_error(test[merchant_name].values, fitted.values))\n",
    "    return math.sqrt(sklearn.metrics.mean_squared_error(data, forecast))\n",
    "\n",
    "rmse(data['y'].iloc[train.shape[0]: train.shape[0] + test.shape[0]], forecast_baseline['yhat'][train.shape[0]: train.shape[0] + test.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to change the file path\n",
    "# here the input file is the forecast result using the best model \n",
    "# ETS for the 5 months which are a input to the prophet\n",
    "# multivariate approach and added as regressors for predictions\n",
    "predict_multivariate = pd.read_csv('Pred_ETS_5months.csv')\n",
    "del predict_multivariate[predict_multivariate.columns[0]]\n",
    "# Using DataFrame.insert() to add a column\n",
    "predict_multivariate.insert(0, \"ds\", stddates[stddates > train_end_date], True)\n",
    "predict_multivariate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs1 = [train, predict_multivariate]\n",
    "# Append the date(ds) column\n",
    "train1 = pd.concat(dfs1).reset_index(drop=True)\n",
    "train1.tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up parameter grid\n",
    "param_grid = {  \n",
    "    'changepoint_prior_scale': [0.001, 0.05, 0.08, 0.5],\n",
    "    'seasonality_prior_scale': [0.01, 1, 5, 10, 12],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}# Generate all combinations of parameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]# Create a list to store MAPE values for each combination\n",
    "mapes = [] \n",
    "# horizon = test period of each fold\n",
    "horizon = '91 days'\n",
    "# initial: training period. (optional. default is 3x of horizon)\n",
    "initial = str(91 * 2) + ' days'  \n",
    "# period: spacing between cutoff dates (optional. default is 0.5x of horizon)\n",
    "period = str(91 * 2) + ' days' \n",
    "\n",
    "# Use cross validation to evaluate all parameters\n",
    "for params in all_params:\n",
    "    # Fit a model using one parameter combination\n",
    "    m = Prophet(**params).fit(train)  \n",
    "    # Cross-validation\n",
    "    df_cv = cross_validation(m, initial=initial, period=period, horizon=horizon)\n",
    "    # Model performance\n",
    "    df_p = performance_metrics(df_cv, rolling_window=1)\n",
    "    # Save model performance metrics\n",
    "    mapes.append(df_p['mape'].values[0])            \n",
    "    \n",
    "# Tuning results\n",
    "tuning_results = pd.DataFrame(all_params)\n",
    "tuning_results['mape'] = mapes# Find the best parameters\n",
    "best_params = all_params[np.argmin(mapes)]\n",
    "#auto_model = Prophet(changepoint_prior_scale=best_params['changepoint_prior_scale'], \n",
    "#                     seasonality_prior_scale=best_params['seasonality_prior_scale'], \n",
    "#                     seasonality_mode=best_params['seasonality_mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)# Fit the model using the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_merchant_ids=[]\n",
    "rmse_error = 0\n",
    "\n",
    "for m in range(10):\n",
    "    # changes the no of merchants added to regressors starting \n",
    "    for l in range(6):\n",
    "        model_multivariate = Prophet(interval_width=0.95, changepoint_prior_scale=0.05, seasonality_prior_scale=0.01, seasonality_mode='additive') #seasonality_mode='multiplicative')\n",
    "        merchant_ids=[]\n",
    "        k = 0\n",
    "        my_input_shuffle = shuffle(merchant_names)\n",
    "        correlation = data_test[my_input_shuffle].corrwith(data_test['y'], method='spearman')   #method{‘pearson’, ‘kendall’, ‘spearman’} or callable\n",
    "        # Add regressor\n",
    "        for i in range(len(merchant_names)):\n",
    "            if(k>l): break\n",
    "            if(correlation[i] > 0.80):\n",
    "                model_multivariate.add_regressor(my_input_shuffle[i], standardize=False)\n",
    "                merchant_ids.append(my_input_shuffle[i])\n",
    "                k+=1\n",
    "\n",
    "        # Fit the model on the training dataset\n",
    "        model_multivariate.fit(train)\n",
    "        df2 = train1[merchant_ids]\n",
    "        future_multivariate = model_multivariate.make_future_dataframe(6, freq='M')\n",
    "        dfs = [future_multivariate, df2]\n",
    "        # Append the regressor values\n",
    "        future_multivariate = pd.concat(dfs, join='inner', axis=1)\n",
    "        # Make prediction\n",
    "        forecast_multivariate = model_multivariate.predict(future_multivariate)\n",
    "        # Visualize the forecast\n",
    "        #model_multivariate.plot(forecast_multivariate)\n",
    "        error_rmse = rmse(data['y'].iloc[21:26], forecast_multivariate['yhat'].iloc[21:26])\n",
    "        if m == 0: rmse_error = error_rmse\n",
    "        if rmse_error > error_rmse : \n",
    "            rmse_error = error_rmse\n",
    "            rmse_merchant_ids.append(merchant_ids)\n",
    "        #print(l,' ',error_rmse)\n",
    "        #print(merchant_ids)\n",
    "        del model_multivariate\n",
    "        del forecast_multivariate\n",
    "        del future_multivariate\n",
    "        del df2\n",
    "        del dfs\n",
    "print('smallest RMSE error: ', rmse_error)\n",
    "print('merchant ids: ', rmse_merchant_ids)\n",
    "#        f.write(' the rmse error is ')\n",
    "#        f.write(str(error_rmse))\n",
    "#        f.write('\\n') smallest error:  535403.1731782097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_merchant_ids=[]\n",
    "rmse_error = 0\n",
    "\n",
    "model_multivariate = Prophet(interval_width=0.99, changepoint_prior_scale=0.05, seasonality_prior_scale=0.01, seasonality_mode='additive') #seasonality_mode='multiplicative')\n",
    "merchant_ids=[]\n",
    "#these are ideal merchant IDS obtained from the above rune which are needed for the multivariate run to get the lowest RMSE error\n",
    "merchant_ids = ['Merchant 828', 'Merchant 156', 'Merchant 483', 'Merchant 13', 'Merchant 242']\n",
    "model_multivariate.add_regressor('Merchant 828', standardize=False)\n",
    "model_multivariate.add_regressor('Merchant 156', standardize=False)\n",
    "model_multivariate.add_regressor('Merchant 483', standardize=False)\n",
    "model_multivariate.add_regressor('Merchant 13', standardize=False)\n",
    "model_multivariate.add_regressor('Merchant 242', standardize=False)\n",
    "\n",
    "# Fit the model on the training dataset\n",
    "model_multivariate.fit(train)\n",
    "df2 = train1[merchant_ids]\n",
    "future_multivariate = model_multivariate.make_future_dataframe(5, freq='M')\n",
    "dfs = [future_multivariate, df2]\n",
    "# Append the regressor values\n",
    "future_multivariate = pd.concat(dfs, join='inner', axis=1)\n",
    "# Make prediction\n",
    "forecast_multivariate = model_multivariate.predict(future_multivariate)\n",
    "# Visualize the forecast\n",
    "model_multivariate.plot(forecast_multivariate)\n",
    "error_rmse = rmse(data['y'].iloc[21:26], forecast_multivariate['yhat'].iloc[21:26])\n",
    "print('smallest RMSE error: ', error_rmse)\n",
    "print('merchant ids: ', merchant_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_multivariate['ds'].iloc[train.shape[0]: train.shape[0] + test.shape[0]], forecast_multivariate['yhat'].iloc[train.shape[0]: train.shape[0] + test.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R56gMfflvua"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHDWii_1qQfw"
   },
   "source": [
    "[1] [Prophet Documentation](https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1_ct3SFOb_RBMAx1G496h1cKd_FOXyJlH",
     "timestamp": 1666949267646
    },
    {
     "file_id": "1Q10bzMzxQLjo7Ltrd3pqJJEsMAKU3vi3",
     "timestamp": 1653007173680
    }
   ]
  },
  "kernelspec": {
   "display_name": "six_project",
   "language": "python",
   "name": "six_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
